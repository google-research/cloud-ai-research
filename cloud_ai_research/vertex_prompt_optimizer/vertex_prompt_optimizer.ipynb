{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_rwhuTSMbRO"
      },
      "source": [
        "#Overview\n",
        "Welcome to Automated Prompt Design (APD)! This Colab showcases APD, a tool that iteratively optimizes prompts to suit a target model (e.g., Gemini-1.5-pro) using target-specific metric(s).\n",
        "\n",
        "Key Use Cases:\n",
        "\n",
        "* Prompt Optimization: Enhance the quality of an initial prompt by refining its structure and content to match the target model's optimal input characteristics.\n",
        "\n",
        "* Prompt Translation: Adapt prompts optimized for one model to work effectively with a different target model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTtKHedrO1Rx"
      },
      "source": [
        "# Step 0: Install packages and libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-Zw72vFORz_"
      },
      "outputs": [],
      "source": [
        "# @title Step 0: Install packages and libraries\n",
        "! pip3 install -U google-cloud-aiplatform -q\n",
        "\n",
        "from logging import disable\n",
        "\n",
        "from google.cloud import storage\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "import gspread\n",
        "\n",
        "import jinja2\n",
        "import jinja2.meta\n",
        "from jinja2 import Environment, BaseLoader\n",
        "import tensorflow.io.gfile as gfile\n",
        "from google.cloud import aiplatform\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import time\n",
        "from IPython.display import display, clear_output, HTML\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import VBox, HBox, Layout\n",
        "from typing import Optional\n",
        "import os\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "from io import StringIO\n",
        "\n",
        "import json\n",
        "import re\n",
        "\n",
        "\n",
        "def authenticate():\n",
        "  auth.authenticate_user()\n",
        "  creds, _ = default()\n",
        "  return gspread.authorize(creds)\n",
        "\n",
        "def is_target_required_metric(eval_metric: str):\n",
        "  return eval_metric in [\n",
        "      \"bleu\",\n",
        "      \"exact_match\",\n",
        "      \"question_answering_correctness\",\n",
        "      \"response_recall\",\n",
        "      \"rouge_1\",\n",
        "      \"rouge_2\",\n",
        "      \"rouge_l\",\n",
        "      \"rouge_l_sum\",\n",
        "      \"tool_name_match\",\n",
        "      \"tool_parameter_key_match\",\n",
        "      \"tool_parameter_kv_match\",\n",
        "  ]\n",
        "\n",
        "def is_run_target_required(eval_metric_types: list[str], source_model:str):\n",
        "  if source_model:\n",
        "    return False\n",
        "\n",
        "  label_required = False\n",
        "  for metric in eval_metric_types:\n",
        "    label_required = label_required or is_target_required_metric(metric)\n",
        "  return label_required\n",
        "\n",
        "def prepare_data(gc, google_sheet_name, dataset_path):\n",
        "  # Open the Google Sheet\n",
        "  try:\n",
        "    sheet = gc.open(google_sheet_name).sheet1  # Adjust the sheet name if necessary\n",
        "  except gspread.SpreadsheetNotFound:\n",
        "    raise ValueError(\n",
        "        f\"Error: {google_sheet_name} doesn't exist in your Google drive.\")\n",
        "\n",
        "  # Read all the data from the sheet\n",
        "  data = sheet.get_all_records()  # Returns a list of dictionaries\n",
        "\n",
        "  # Write data to a JSONL file\n",
        "  with gfile.GFile(dataset_path, 'w') as jsonl_file:  # Adjust path as needed\n",
        "      for record in data:\n",
        "          jsonl_file.write(json.dumps(record) + '\\n')\n",
        "\n",
        "\n",
        "def validate_prompt_and_data(template: str,\n",
        "                             dataset_path: str,\n",
        "                             placeholder_to_content: str,\n",
        "                             label_enforced: bool):\n",
        "  placeholder_to_content = json.loads(placeholder_to_content)\n",
        "  _TARGET_KEY = \"target\"\n",
        "  with gfile.GFile(dataset_path, 'r') as f:\n",
        "    data = [json.loads(line) for line in f.readlines()]\n",
        "\n",
        "  env = jinja2.Environment()\n",
        "  try:\n",
        "    parsed_content = env.parse(template)\n",
        "  except jinja2.exceptions.TemplateSyntaxError as e:\n",
        "    raise ValueError(f\"Invalid template: {template}\") from e\n",
        "\n",
        "  template_variables = jinja2.meta.find_undeclared_variables(\n",
        "      parsed_content)\n",
        "  extra_keys = set()\n",
        "  for ex in data:\n",
        "    ex.update(placeholder_to_content)\n",
        "    missing_keys = [key for key in template_variables if key not in ex]\n",
        "    extra_keys.update([key for key in ex if key not in template_variables])\n",
        "    if label_enforced:\n",
        "      if _TARGET_KEY not in ex:\n",
        "        raise ValueError(\n",
        "            f\"The example {ex} doesn't have a key corresponding to the target var: {_TARGET_KEY}\")\n",
        "      if not ex[_TARGET_KEY]:\n",
        "        raise ValueError(\n",
        "            f\"The followig example has an empty target: {ex}\")\n",
        "    if missing_keys:\n",
        "      raise ValueError(\n",
        "          f\"The example {ex} doesn't have a key corresponding to following template vars: {missing_keys}\")\n",
        "  if extra_keys:\n",
        "    raise Warning(\n",
        "        f\"Warning: exrta keys in the examples not used in the context/task template {extra_keys}\")\n",
        "\n",
        "\n",
        "def run_custom_job(\n",
        "    display_name: str,\n",
        "    container_uri: str,\n",
        "    container_args: dict[str, str],\n",
        ") -\u003e None:\n",
        "  \"\"\"A sample to create custom jobs.\"\"\"\n",
        "  worker_pool_specs = [{\n",
        "      'replica_count': 1,\n",
        "      'container_spec': {\n",
        "          'image_uri': container_uri,\n",
        "          'args': ['--%s=%s' % (k, v) for k, v in container_args.items()],\n",
        "      },\n",
        "      'machine_spec': {\n",
        "          'machine_type': 'n1-standard-4',\n",
        "      },\n",
        "  }]\n",
        "\n",
        "  custom_job = aiplatform.CustomJob(\n",
        "      display_name=display_name,\n",
        "      worker_pool_specs=worker_pool_specs,\n",
        "  )\n",
        "  custom_job.submit()\n",
        "  return custom_job\n",
        "\n",
        "\n",
        "def run_apd(config, bucket_uri, display_name):\n",
        "  print(f\"\\n\\nJob display name: {display_name}\")\n",
        "  _VERSION = 'preview_v1_0'\n",
        "  _CONTAINER_URI = f'us-docker.pkg.dev/vertex-ai-restricted/builtin-algorithm/apd'\n",
        "  config_path = f'{bucket_uri}/{display_name}/input_config.json'\n",
        "\n",
        "  with gfile.GFile(config_path, \"w\") as f:\n",
        "    json.dump(config, f)\n",
        "\n",
        "  aiplatform.init(\n",
        "      project=config[\"project\"],\n",
        "      location=config[\"target_model_location\"],\n",
        "      staging_bucket=f\"{bucket_uri}/{display_name}\",\n",
        "  )\n",
        "\n",
        "  return run_custom_job(\n",
        "      display_name=display_name,\n",
        "      container_uri=f'{_CONTAINER_URI}:{_VERSION}',\n",
        "      container_args={\"config\":config_path},\n",
        "  )\n",
        "\n",
        "\n",
        "def update_best_display(df,\n",
        "                        textarea: widgets.Textarea,\n",
        "                        best_score_label: widgets.Label,\n",
        "                        eval_metric: str):\n",
        "  df['score'] = df[f'metrics.{eval_metric}/mean']\n",
        "\n",
        "  # # Step 4: Find the row with the maximum 'mean' value\n",
        "  best_template = df.loc[df[\"score\"].argmax(), \"prompt\"]\n",
        "  best_score = df.loc[df[\"score\"].argmax(), \"score\"]\n",
        "  original_score = df.loc[0, \"score\"]\n",
        "  last_step = df.loc[df[\"step\"].argmax(), \"step\"]\n",
        "\n",
        "  def placeholder_llm():\n",
        "      return \"{{llm()}}\"\n",
        "\n",
        "  env = Environment(loader=BaseLoader())\n",
        "  env.globals['llm'] = placeholder_llm\n",
        "\n",
        "  best_template = best_template.replace(\"store('answer', llm())\", \"llm()\")\n",
        "  textarea.value = best_template\n",
        "  improvement = best_score - original_score\n",
        "  no_improvement_str = (\n",
        "      \"\\nNo better template is found yet.\" if not improvement else \"\")\n",
        "  best_score_label.value = (f\"Score: {best_score}\"\n",
        "    f\" Improvement: {improvement: .3f} {no_improvement_str}\")\n",
        "\n",
        "\n",
        "def generate_dataframe(filename: str)-\u003epd.DataFrame:\n",
        "    if not gfile.exists(filename):\n",
        "      return pd.DataFrame()\n",
        "\n",
        "    with gfile.GFile(filename, \"r\") as f:\n",
        "      try:\n",
        "        data = json.load(f)\n",
        "      except:\n",
        "        return pd.DataFrame()\n",
        "      return pd.json_normalize(data)\n",
        "\n",
        "\n",
        "def left_aligned_df_html(df):\n",
        "    \"\"\"Displays a Pandas DataFrame in Colab with left-aligned values.\"\"\"\n",
        "\n",
        "    # Convert to HTML table, but keep the HTML in a variable\n",
        "    html_table = df.to_html(index=False, classes=\"left-aligned\")\n",
        "\n",
        "    # Add CSS styling to left-align table data cells and override default styles\n",
        "    styled_html = f\"\"\"\n",
        "    \u003cstyle\u003e\n",
        "        .left-aligned td, .left-aligned th {{ text-align: left !important; }}\n",
        "    \u003c/style\u003e\n",
        "    {html_table}\n",
        "    \"\"\"\n",
        "\n",
        "    # Display the styled HTML table\n",
        "    return HTML(styled_html)\n",
        "\n",
        "\n",
        "def extract_top_level_function_name(source_code: str) -\u003e str:\n",
        "    match = re.search(r'^def\\s+([a-zA-Z_]\\w*)\\s*\\(', source_code, re.MULTILINE)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    return None\n",
        "\n",
        "\n",
        "class ProgressForm():\n",
        "  def __init__(self):\n",
        "    self.instruction_progress_bar = None\n",
        "    self.instruction_display = None\n",
        "    self.instruction_best = None\n",
        "    self.instruction_score = None\n",
        "\n",
        "    self.demo_progress_bar = None\n",
        "    self.demo_display = None\n",
        "    self.demo_best = None\n",
        "    self.demo_score = None\n",
        "\n",
        "    self.job_state_display = None\n",
        "\n",
        "    self.instruction_df = None\n",
        "    self.demo_df = None\n",
        "\n",
        "    self.started = False\n",
        "\n",
        "\n",
        "  def init(self, params):\n",
        "    self.job_state_display = display(HTML('\u003cspan\u003eJob State: Not Started!\u003c/span\u003e'), display_id=True)\n",
        "    self.status_display = display(HTML(''), display_id=True)\n",
        "\n",
        "\n",
        "    if params[\"optimization_mode\"] in [\"instruction\", \"instruction_and_demo\"]:\n",
        "      self.instruction_progress_bar, self.instruction_display, self.instruction_best, self.instruction_score = (\n",
        "          self.create_progress_ui(\"Instruction\",\n",
        "                             params[\"num_steps\"]))\n",
        "\n",
        "    if params[\"optimization_mode\"] in [\"demonstration\", \"instruction_and_demo\"]:\n",
        "      self.demo_progress_bar, self.demo_display, self.demo_best, self.demo_score = (\n",
        "          self.create_progress_ui(\"Demonstration\",\n",
        "                             params[\"num_demo_set_candidates\"]))\n",
        "\n",
        "    eval_metric = 'composite_metric'\n",
        "    if len(params[\"eval_metrics_types\"]) == 1:\n",
        "      eval_metric = params[\"eval_metrics_types\"][0]\n",
        "\n",
        "    if eval_metric != 'composite_metric' and \"custom_metric_source_code\" in params:\n",
        "      self.eval_metric = extract_top_level_function_name(\n",
        "          params[\"custom_metric_source_code\"])\n",
        "    else:\n",
        "      self.eval_metric = eval_metric\n",
        "\n",
        "    self.output_path = params[\"output_path\"]\n",
        "    self.started = True\n",
        "\n",
        "\n",
        "  def update_progress(self,\n",
        "                      progress_bar: widgets.IntProgress,\n",
        "                      templates_file: str,\n",
        "                      df: Optional[pd.DataFrame],\n",
        "                      df_display,\n",
        "                      best_textarea,\n",
        "                      best_score,\n",
        "                      eval_metric):\n",
        "\n",
        "    def get_last_step(df: pd.DataFrame):\n",
        "      if df.empty:\n",
        "        return -1\n",
        "      return int(df[\"step\"].max())\n",
        "\n",
        "    if progress_bar is None or df is None:\n",
        "      return pd.DataFrame()\n",
        "\n",
        "    new_df = generate_dataframe(templates_file)\n",
        "\n",
        "    last_step = get_last_step(df)\n",
        "    new_last_step = get_last_step(new_df)\n",
        "    if new_last_step \u003e last_step:\n",
        "      df_display.update(left_aligned_df_html(new_df))\n",
        "      update_best_display(new_df, best_textarea, best_score, eval_metric)\n",
        "      progress_bar.value = progress_bar.value + new_last_step - last_step\n",
        "\n",
        "    return new_df\n",
        "\n",
        "\n",
        "  def create_progress_ui(self, opt_mode: str, num_opt_steps: int):\n",
        "    print(f\"\\n\\n{opt_mode} Optimization\")\n",
        "    progress_bar = widgets.IntProgress(value=0,\n",
        "                                      min=0,\n",
        "                                      max=num_opt_steps,\n",
        "                                      step=1,\n",
        "                                      description='Progress')\n",
        "    display(progress_bar)\n",
        "    print(\"\\nGenerated Templates:\")\n",
        "    templates_display = display(\n",
        "        \"No template is evaluated yet!\", display_id=True)\n",
        "\n",
        "    print(\"\\nBest Template so far:\")\n",
        "    best_textarea = widgets.Textarea(\n",
        "        value=\"NA\",\n",
        "        disabled=False,\n",
        "        layout=widgets.Layout(width='80%', height='150px')\n",
        "    )\n",
        "    display(best_textarea)\n",
        "\n",
        "    best_score = widgets.Label(\n",
        "        value=\"Score: NA Improvement: NA\"\n",
        "    )\n",
        "    display(best_score)\n",
        "\n",
        "    return progress_bar, templates_display, best_textarea, best_score\n",
        "\n",
        "\n",
        "  def monitor_progress(self, job, params):\n",
        "    if not self.started:\n",
        "      self.init(params)\n",
        "\n",
        "    self.job_state_display.update(HTML(f'\u003cspan\u003eJob State: {job.state.name}\u003c/span\u003e'))\n",
        "\n",
        "    # Initial display of the dataframe\n",
        "    instruction_templates_file = f'{self.output_path}/instruction/templates.json'\n",
        "    demo_templates_file = f'{self.output_path}/demonstration/templates.json'\n",
        "\n",
        "    if not job.done():\n",
        "      self.instruction_df = self.update_progress(\n",
        "          self.instruction_progress_bar,\n",
        "          instruction_templates_file,\n",
        "          self.instruction_df,\n",
        "          self.instruction_display,\n",
        "          self.instruction_best,\n",
        "          self.instruction_score,\n",
        "          self.eval_metric)\n",
        "      self.demo_df = self.update_progress(\n",
        "          self.demo_progress_bar,\n",
        "          demo_templates_file,\n",
        "          self.demo_df,\n",
        "          self.demo_display,\n",
        "          self.demo_best,\n",
        "          self.demo_score,\n",
        "          self.eval_metric)\n",
        "      return True\n",
        "\n",
        "    if job.state.name != \"JOB_STATE_SUCCEEDED\":\n",
        "      errors = [f\"Error: Job failed with error {job.error}.\"]\n",
        "      for err_file in [f'{self.output_path}/instruction/error.json',\n",
        "                      f'{self.output_path}/demonstration/error.json']:\n",
        "        if gfile.exists(err_file):\n",
        "          with gfile.GFile(err_file, \"r\") as f:\n",
        "            error_json = json.load(f)\n",
        "          errors.append(f\"Detailed error: {error_json}\")\n",
        "          errors.append(f\"Please feel free to send {err_file} to the APD team to help resolving the issue.\")\n",
        "\n",
        "      errors.append(f\"All the templates found before failure can be found under {self.output_path}\")\n",
        "      errors.append(f\"Please consider reruning to make sure the failure is intransient.\")\n",
        "      err = \"\\n\".join(errors)\n",
        "      self.status_display.update(HTML(f'\u003cspan style=\"color: red;\"\u003e{err}\u003c/span\u003e'))\n",
        "    else:\n",
        "      self.status_display.update(\n",
        "          HTML(f'\u003cspan style=\"color: green;\"\u003eJob succeeded!\u003c/span\u003e \u003cspan\u003eAll the artifacts can be found under {self.output_path}\u003c/span\u003e'))\n",
        "    return False\n",
        "\n",
        "def display_dataframe(df: pd.DataFrame):\n",
        "  # Function to wrap text in a scrollable div\n",
        "  def wrap_in_scrollable_div(text):\n",
        "      return f'\u003cdiv class=\"scrollable\"\u003e{text}\u003c/div\u003e'\n",
        "\n",
        "  # Apply the function to every cell using the format method\n",
        "  styled_html = df.style.format(wrap_in_scrollable_div).to_html(index=False)\n",
        "\n",
        "  # Display the HTML in the notebook\n",
        "  display(HTML(styled_html))\n",
        "\n",
        "\n",
        "def split_gcs_path(gcs_path: str):\n",
        "    \"\"\"Splits a full GCS path into bucket name and prefix.\"\"\"\n",
        "    if gcs_path.startswith('gs://'):\n",
        "        path_without_scheme = gcs_path[5:]  # Remove the 'gs://' part\n",
        "        parts = path_without_scheme.split('/', 1)\n",
        "        bucket_name = parts[0]\n",
        "        prefix = parts[1] if len(parts) \u003e 1 else ''\n",
        "        return bucket_name, prefix\n",
        "    else:\n",
        "        raise ValueError(\"Invalid GCS path. Must start with 'gs://'\")\n",
        "\n",
        "\n",
        "def list_gcs_objects(full_gcs_path: str):\n",
        "    \"\"\"Lists all the objects in the given GCS path.\"\"\"\n",
        "    bucket_name, prefix = split_gcs_path(full_gcs_path)\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blobs = bucket.list_blobs(prefix=prefix)  # List all objects that start with the prefix\n",
        "\n",
        "    return [blob.name for blob in blobs]\n",
        "\n",
        "\n",
        "def find_directories_with_files(full_gcs_path, required_files):\n",
        "    \"\"\"Finds directories containing specific files under the given full GCS path.\"\"\"\n",
        "    bucket_name, prefix = split_gcs_path(full_gcs_path)\n",
        "    all_paths = list_gcs_objects(f'gs://{bucket_name}/{prefix}')\n",
        "    directories = set()\n",
        "\n",
        "    # Create a dictionary to track files found in each directory\n",
        "    file_presence = {}\n",
        "    for path in all_paths:\n",
        "        directory = '/'.join(path.split('/')[:-1])  # Get the directory part of the path\n",
        "        filename = path.split('/')[-1]  # Get the filename part of the path\n",
        "        if directory:\n",
        "            if directory not in file_presence:\n",
        "                file_presence[directory] = set()\n",
        "            file_presence[directory].add(filename)\n",
        "\n",
        "    # Check which directories have all required files\n",
        "    for directory, files in file_presence.items():\n",
        "        if all(file in files for file in required_files):\n",
        "            directories.add(f\"gs://{bucket_name}/{directory}\")\n",
        "\n",
        "    return list(directories)\n",
        "\n",
        "def extract_metric_name(metric_string: str):\n",
        "    # Use a regular expression to find the metric name\n",
        "    match = re.search(r'\\.(\\w+)/', metric_string)\n",
        "    # Return the matched group if found\n",
        "    return match.group(1) if match else metric_string\n",
        "\n",
        "\n",
        "def read_file_from_gcs(filename: str):\n",
        "    with gfile.GFile(filename, \"r\") as f:\n",
        "      return f.read()\n",
        "\n",
        "\n",
        "def process_results(df: pd.DataFrame):\n",
        "  columns_to_drop = []\n",
        "  # Dropping columns that could be confusing.\n",
        "  for col in df.columns:\n",
        "    if \"confidence\" in col:\n",
        "      columns_to_drop.append(col)\n",
        "    if \"raw_eval_resp\" in col:\n",
        "      columns_to_drop.append(col)\n",
        "    if col == \"instruction\":\n",
        "      columns_to_drop.append(col)\n",
        "    if col == \"context\":\n",
        "      columns_to_drop.append(col)\n",
        "  return df.drop(columns=columns_to_drop)\n",
        "\n",
        "\n",
        "class ResultsUI:\n",
        "  def __init__(self, path: str):\n",
        "    required_files = ['eval_results.json', 'templates.json']\n",
        "    runs = find_directories_with_files(path, required_files)\n",
        "\n",
        "    self.run_label = widgets.Label('Select Run:')\n",
        "    self.run_dropdrown = widgets.Dropdown(options=runs,\n",
        "                                          value=runs[0],\n",
        "                                          layout=widgets.Layout(width='200px'))\n",
        "    self.run_dropdrown.observe(self.display_run_handler, names='value')\n",
        "\n",
        "    # Create a label widget for the description\n",
        "    self.dropdown_description = widgets.Label('Select Template:')\n",
        "    self.template_dropdown = widgets.Dropdown(options=[],\n",
        "                                           value=None,\n",
        "                                           layout=widgets.Layout(width='400px'),\n",
        "                                           disabled=True\n",
        "                                           )\n",
        "    self.template_dropdown.observe(self.display_template_handler, names='value')\n",
        "    self.results_output = widgets.Output(\n",
        "        layout=widgets.Layout(height='600px',\n",
        "                              overflow='auto',\n",
        "                              margin='20px 0px 0px 0px'))\n",
        "    self.display_run(runs[0])\n",
        "\n",
        "  def display_template_handler(self, change):\n",
        "    if change['new'] is None:\n",
        "      return\n",
        "    df_index = int(change['new'].split(\" \")[1])\n",
        "    self.display_eval_results(df_index)\n",
        "\n",
        "  def display_run_handler(self, change):\n",
        "    if change['new'] is None:\n",
        "      return\n",
        "\n",
        "    path = change['new']\n",
        "    self.display_run(path)\n",
        "\n",
        "  def display_run(self, path: str):\n",
        "    self.run_dropdrown.disabled = True\n",
        "    filename = f\"{path}/eval_results.json\"\n",
        "    eval_results = json.loads(read_file_from_gcs(filename))\n",
        "\n",
        "    filename = f\"{path}/templates.json\"\n",
        "    templates = json.loads(read_file_from_gcs(filename))\n",
        "\n",
        "    if len(templates) == len(eval_results):\n",
        "      offset = 0\n",
        "    elif len(templates) == len(eval_results) + 1:\n",
        "      # In some setups it is possible to have 1 more template than results.\n",
        "      offset = 1\n",
        "    else:\n",
        "      raise ValueError(f\"Number of templates doesn't match number of eval results {len(templates)} vs {len(eval_results)}\")\n",
        "    self.templates = [pd.json_normalize(template) for template in templates[offset:]]\n",
        "    metric_columns = [col for col in self.templates[0].columns if \"metric\" in col]\n",
        "\n",
        "    self.eval_results = [process_results(pd.read_json(StringIO(result[\"metrics_table\"]))) for result in eval_results]\n",
        "    options = []\n",
        "    for i, template in enumerate(self.templates):\n",
        "      metrics = []\n",
        "      for col in metric_columns:\n",
        "        value = template[col].tolist()[0]\n",
        "        short_col = extract_metric_name(col)\n",
        "        metrics.append(f\"{short_col}: {value}\")\n",
        "      metrics_str = \" \".join(metrics)\n",
        "      options.append(f\"Template {i} {metrics_str}\")\n",
        "\n",
        "    self.template_dropdown.disabled = False\n",
        "    self.template_dropdown.options = options\n",
        "    self.run_dropdrown.disabled = False\n",
        "\n",
        "  def display_eval_results(self, index):\n",
        "    with self.results_output:\n",
        "      self.results_output.clear_output(wait=True)  # Clear previous output\n",
        "      display_dataframe(self.templates[index])\n",
        "      print()\n",
        "      display_dataframe(self.eval_results[index])\n",
        "\n",
        "  def get_container(self):\n",
        "    return widgets.VBox([self.run_label,\n",
        "                         self.run_dropdrown,\n",
        "                         self.dropdown_description,\n",
        "                         self.template_dropdown,\n",
        "                         self.results_output])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p59jd5rOp4q"
      },
      "source": [
        "# Step 1: Configure your prompt template\n",
        "Prompts consist of two key parts:\n",
        "* System Instruction (SI) Template: A fixed instruction shared across all queries for a given task.\n",
        "* Task/Context Template: A dynamic part that changes based on the task.\n",
        "\n",
        "APD enables the translation and optimization of the System Instruction Template, while the Task/Context Template remains essential for evaluating different SI templates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJG1pVZO317x"
      },
      "outputs": [],
      "source": [
        "INSTRUCTION_TEMPLATE =\"Answer the following question. Let's think step by step.\\n\" # @param {type:\"string\"}\n",
        "CONTEXT_TASK_TEMPLATE = \"Question: {{question}}\\n\\nAnswer:{{target}}\" # @param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y-cmg0TQP6v"
      },
      "source": [
        "# Step 2: Input your data\n",
        "To optimize the model, provide a CSV or JSONL file containing labeled validation samples\n",
        "* Focus on examples that specifically demonstrate the issues you want to address.\n",
        "* Recommendation: Use 50-100 distinct samples for reliable results. However, the tool can still be effective with as few as 5 samples.\n",
        "\n",
        "For prompt translation:\n",
        "* Consider using the source model to label examples that the target model struggles with, helping to identify areas for improvement.\n",
        "\n",
        "Learn more about setting up your CSV or JSONL file as input [TODO: add link]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfgi_oR6tTIB"
      },
      "outputs": [],
      "source": [
        "# @markdown **Project setup**: \u003cbr/\u003e\n",
        "PROJECT_ID = \"vertex-research-solutions\" # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\" # @param {type:\"string\"}\n",
        "# @markdown * GCS path of your bucket, e.g., gs://prompt_translation_demo, used to store all artifacts.\n",
        "BUCKET = \"gs://prompt_design_demo\" # @param {type:\"string\"}\n",
        "# @markdown * Specify the input data path inside your bucket.\n",
        "INPUT_DATA_PATH = \"movie_recommendation.jsonl\" # @param {type:\"string\"}\n",
        "\n",
        "# # @markdown **Data path**: \u003cbr/\u003e\n",
        "# GOOGLE_SHEET_NAME = \"example_apd_input\" # @param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucebZHkHRxKH"
      },
      "source": [
        "# Step 3: Configure optimization settings\n",
        "The optimization configs are defaulted to the values that are most commonly used and which we recommend using initially. Refer here [TODO:add link] to learn more about the different parameters settings and how to best utilize them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2R3P8mMvK9q"
      },
      "outputs": [],
      "source": [
        "TARGET_MODEL = \"gemini-1.5-flash-001\" # @param [\"gemini-1.0-pro-001\", \"gemini-1.0-pro-002\", \"gemini-1.5-flash-001\", \"gemini-1.5-pro-001\", \"gemini-1.0-ultra-001\", \"text-bison@001\", \"text-bison@002\", \"text-bison32k@002\", \"text-unicorn@001\"]\n",
        "# @markdown * If set, it will be used to generate ground truth responses for the input examples. This is useful to migrate the prompt from a source model.\n",
        "SOURCE_MODEL = \"\" # @param [\"\", \"gemini-1.0-pro-001\", \"gemini-1.0-pro-002\", \"gemini-1.5-flash-001\", \"gemini-1.5-pro-001\", \"gemini-1.0-ultra-001\", \"text-bison@001\", \"text-bison@002\", \"text-bison32k@002\", \"text-unicorn@001\"]\n",
        "OPTIMIZATION_MODE = \"instruction_and_demo\" # @param [\"instruction\", \"demonstration\", \"instruction_and_demo\"]\n",
        "OPTIMIZATION_METRIC = \"question_answering_correctness\" # @param [\"bleu\", \"coherence\", \"exact_match\", \"fluency\", \"groundedness\", \"text_quality\", \"verbosity\", \"rouge_1\", \"rouge_2\", \"rouge_l\", \"rouge_l_sum\", \"safety\", \"question_answering_correctness\", \"question_answering_quality\", \"summarization_quality\", \"tool_name_match\", \"tool_parameter_key_match\", \"tool_parameter_kv_match\", \"tool_call_valid\"] {type:\"string\"}\n",
        "# @markdown **Instruction Optimization Configs**: \u003cbr/\u003e\n",
        "NUM_INST_OPTIMIZATION_STEPS = 10 # @param {type:\"integer\"}\n",
        "# @markdown * Number of prompt templates generated and evaluated at each optimization step.\n",
        "NUM_TEMPLATES_PER_STEP = 2 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown **Demonstration Optimization Configs**: \u003cbr/\u003e\n",
        "NUM_DEMO_OPTIMIZATION_STEPS = 10 # @param {type:\"integer\"}\n",
        "# @markdown * Number of the demonstrations to include in each prompt.\n",
        "NUM_DEMO_PER_PROMPT = 3 # @param {type:\"integer\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kO7fO0qTSNLs"
      },
      "source": [
        "# Step 4: Configure advanced optimization settings [Optional]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRHHTpaV4Xyo"
      },
      "outputs": [],
      "source": [
        "# @markdown **Model Configs**: \u003cbr/\u003e\n",
        "TARGET_MODEL_QPS = 3 # @param {type:\"integer\"}\n",
        "SOURCE_MODEL_QPS = 3 # @param {type:\"integer\"}\n",
        "# @markdown * The model used to generated alternative prompts in the instruction optimization mode.\n",
        "OPTIMIZER_MODEL = \"gemini-1.5-flash-001\" # @param [\"gemini-1.0-pro-001\", \"gemini-1.0-pro-002\", \"gemini-1.5-flash-001\", \"gemini-1.5-pro-001\", \"gemini-1.0-ultra-001\", \"text-bison@001\", \"text-bison@002\", \"text-bison32k@002\", \"text-unicorn@001\"]\n",
        "OPTIMIZER_MODEL_QPS = 3 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown * The QPS for calling the eval model, which is currently gemini-1.5-pro-001.\n",
        "EVAL_MODEL_QPS = 3 # @param {type:\"integer\"}\n",
        "# @markdown **Multi-metric Configs**: \u003cbr/\u003e\n",
        "# @markdown Use this section only if you need more than one metric for optimization. This will override the metric you picked above.\n",
        "\n",
        "OPTIMIZATION_METRIC_1 = \"NA\" # @param [\"NA\", \"bleu\", \"coherence\", \"exact_match\", \"fluency\", \"groundedness\", \"text_quality\", \"verbosity\", \"rouge_1\", \"rouge_2\", \"rouge_l\", \"rouge_l_sum\", \"safety\", \"question_answering_correctness\", \"question_answering_quality\", \"summarization_quality\", \"tool_name_match\", \"tool_parameter_key_match\", \"tool_parameter_kv_match\", \"tool_call_valid\"] {type:\"string\"}\n",
        "OPTIMIZATION_METRIC_1_WEIGHT = 0.0 # @param {type:\"number\"}\n",
        "OPTIMIZATION_METRIC_2 = \"NA\" # @param [\"NA\", \"bleu\", \"coherence\", \"exact_match\", \"fluency\", \"groundedness\", \"text_quality\", \"verbosity\", \"rouge_1\", \"rouge_2\", \"rouge_l\", \"rouge_l_sum\", \"safety\", \"question_answering_correctness\", \"question_answering_quality\", \"summarization_quality\", \"tool_name_match\", \"tool_parameter_key_match\", \"tool_parameter_kv_match\", \"tool_call_valid\"] {type:\"string\"}\n",
        "OPTIMIZATION_METRIC_2_WEIGHT = 0.0 # @param {type:\"number\"}\n",
        "OPTIMIZATION_METRIC_3 = \"NA\" # @param [\"NA\", \"bleu\", \"coherence\", \"exact_match\", \"fluency\", \"groundedness\", \"text_quality\", \"verbosity\", \"rouge_1\", \"rouge_2\", \"rouge_l\", \"rouge_l_sum\", \"safety\", \"question_answering_correctness\", \"question_answering_quality\", \"summarization_quality\", \"tool_name_match\", \"tool_parameter_key_match\", \"tool_parameter_kv_match\", \"tool_call_valid\"] {type:\"string\"}\n",
        "OPTIMIZATION_METRIC_3_WEIGHT = 0.0 # @param {type:\"number\"}\n",
        "METRIC_AGGREGATION_TYPE = \"weighted_sum\" # @param [\"weighted_sum\", \"weighted_average\"]\n",
        "# @markdown **Misc Configs**: \u003cbr/\u003e\n",
        "# @markdown * This variable is used for long prompt optimization to not optimize parts of prompt identified by placeholders. It provides a mapping from the placeholder variables to their content. See link for details.\n",
        "PLACEHOLDER_TO_VALUE = \"{}\" # @param\n",
        "# @markdown * This variable determines the format of the output for the target model. See link for details.\n",
        "RESPONSE_MIME_TYPE = \"application/json\" # @param [\"text/plain\", \"application/json\"]\n",
        "# TODO: add the description for the language.\n",
        "TARGET_LANGUAGE = \"English\" # @param [\"English\", \"French\", \"German\", \"Hebrew\", \"Hindi\", \"Japanese\", \"Korean\", \"Portuguese\", \"Simplified Chinese\", \"Spanish\", \"Traditional Chinese\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7Mgb0EHSSFk"
      },
      "source": [
        "# Step 5: Run Prompt Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8NvNLTfxPTf"
      },
      "outputs": [],
      "source": [
        "_VERSION = 'preview_v1_0'\n",
        "_CONTAINER_URI = f'us-docker.pkg.dev/vertex-ai-restricted/builtin-algorithm/apd'\n",
        "\n",
        "\n",
        "timestamp = datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
        "display_name = f\"pt_{timestamp}\"\n",
        "\n",
        "in_colab_enterprise = \"GOOGLE_CLOUD_PROJECT\" in os.environ\n",
        "if not in_colab_enterprise:\n",
        "  gc = authenticate()\n",
        "\n",
        "label_enforced = is_run_target_required([OPTIMIZATION_METRIC,\n",
        "                                          OPTIMIZATION_METRIC_1,\n",
        "                                          OPTIMIZATION_METRIC_2,\n",
        "                                          OPTIMIZATION_METRIC_3],\n",
        "                                        SOURCE_MODEL)\n",
        "# TODO: ADD checking the file exsits.\n",
        "input_data_path = f\"{BUCKET}/{INPUT_DATA_PATH}\"\n",
        "validate_prompt_and_data(\n",
        "    \"\\n\".join([INSTRUCTION_TEMPLATE, CONTEXT_TASK_TEMPLATE]),\n",
        "    input_data_path, PLACEHOLDER_TO_VALUE, label_enforced)\n",
        "\n",
        "output_path = f'{BUCKET}/{display_name}'\n",
        "\n",
        "params = {\n",
        "    'project': PROJECT_ID,\n",
        "    'num_steps': NUM_INST_OPTIMIZATION_STEPS,\n",
        "    'prompt_template': INSTRUCTION_TEMPLATE,\n",
        "    'demo_and_query_template': CONTEXT_TASK_TEMPLATE,\n",
        "    'target_model': TARGET_MODEL,\n",
        "    'target_model_qps': TARGET_MODEL_QPS,\n",
        "    'target_model_location': LOCATION,\n",
        "    'source_model': SOURCE_MODEL,\n",
        "    'source_model_qps': SOURCE_MODEL_QPS,\n",
        "    'source_model_location': LOCATION,\n",
        "    'eval_model': EVAL_MODEL,\n",
        "    'eval_model_qps': EVAL_MODEL_QPS,\n",
        "    'eval_model_location': LOCATION,\n",
        "    'optimization_mode': OPTIMIZATION_MODE,\n",
        "    'num_demo_set_candidates': NUM_DEMO_OPTIMIZATION_STEPS,\n",
        "    'demo_set_size': NUM_DEMO_PER_PROMPT,\n",
        "    'aggregation_type': METRIC_AGGREGATION_TYPE,\n",
        "    'data_limit': 50,\n",
        "    'optimizer_model': OPTIMIZER_MODEL,\n",
        "    'optimizer_model_qps': OPTIMIZER_MODEL_QPS,\n",
        "    'optimizer_model_location': LOCATION,\n",
        "    'num_template_eval_per_step': NUM_TEMPLATES_PER_STEP,\n",
        "    'input_data_path': input_data_path,\n",
        "    'output_path': output_path,\n",
        "    'response_mime_type': RESPONSE_MIME_TYPE,\n",
        "    'language': TARGET_LANGUAGE,\n",
        "    'placeholder_to_content': json.loads(PLACEHOLDER_TO_VALUE),\n",
        "}\n",
        "\n",
        "if OPTIMIZATION_METRIC_1 == \"NA\":\n",
        "  params['eval_metrics_types'] = [OPTIMIZATION_METRIC]\n",
        "  params['eval_metrics_weights'] = [1.0]\n",
        "else:\n",
        "  metrics = []\n",
        "  weights = []\n",
        "  for metric in [OPTIMIZATION_METRIC_1,\n",
        "                 OPTIMIZATION_METRIC_2,\n",
        "                 OPTIMIZATION_METRIC_3]:\n",
        "    if metric == \"NA\":\n",
        "      break\n",
        "    metrics.append(metric)\n",
        "    weights.append(OPTIMIZATION_METRIC_1_WEIGHT)\n",
        "  params['eval_metrics_types'] = metrics\n",
        "  params['eval_metrics_weights'] = weights\n",
        "\n",
        "# TODO set the cutom metric source code?\n",
        "# if custom_metric_source_code is not None:\n",
        "#   params['custom_metric_source_code'] = custom_metric_source_code\n",
        "\n",
        "job = run_apd(params, BUCKET, display_name)\n",
        "print(f\"Job ID: {job.name}\")\n",
        "\n",
        "progress_form = ProgressForm()\n",
        "while progress_form.monitor_progress(job, params):\n",
        "  time.sleep(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lo5mcTzwSgBP"
      },
      "source": [
        "# Step 6: Inspect the Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1x6HSty759jY"
      },
      "outputs": [],
      "source": [
        "RESULT_PATH = \"gs://prompt_translation_demo\" # @param {type:\"string\"}\n",
        "results_ui = ResultsUI(RESULT_PATH)\n",
        "\n",
        "results_df_html = \"\"\"\n",
        "\u003cstyle\u003e\n",
        "  .scrollable {\n",
        "    width: 100%;\n",
        "    height: 80px;\n",
        "    overflow-y: auto;\n",
        "    overflow-x: hidden;  /* Hide horizontal scrollbar */\n",
        "  }\n",
        "  tr:nth-child(odd) {\n",
        "    background: var(--colab-highlighted-surface-color);\n",
        "  }\n",
        "  tr:nth-child(even) {\n",
        "    background-color: var(--colab-primary-surface-color);\n",
        "  }\n",
        "  th {\n",
        "    background-color: var(--colab-highlighted-surface-color);\n",
        "  }\n",
        "\u003c/style\u003e\n",
        "\"\"\"\n",
        "\n",
        "display(HTML(results_df_html))\n",
        "display(results_ui.get_container())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1MfXNL9PpH6D8re4KyLZ00Owc7Rk65sye",
          "timestamp": 1724857541568
        },
        {
          "file_id": "1J5bYxXXzcEVKhH8dX6_tlIBigP1X0zEU",
          "timestamp": 1724253156774
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
